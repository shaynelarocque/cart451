<!DOCTYPE html>
<html>

<head>
   <meta charset="UTF-8">
   <title>Shayne La Rocque CART 451 Weekly Reading Responses</title>
   <style>
      body {
         font-family: 'Arial', sans-serif;
         background-color: #f4f4f4;
         color: #333;
         line-height: 1.6;
         padding: 20px;
         max-width: 800px;
         margin: 40px auto;
         border-radius: 8px;
         box-shadow: 0px 0px 15px rgba(0, 0, 0, 0.1);
         background-color: #fff;
      }

      h1 {
         color: #444;
         border-bottom: 2px solid #e4e4e4;
         padding-bottom: 10px;
         margin-bottom: 20px;
      }

      h3 {
         color: #555;
         margin-top: 25px;
         margin-bottom: 15px;
      }

      p {
         margin-bottom: 20px;
         font-size: 16px;
      }

      /* Add some responsive design */
      @media screen and (max-width: 768px) {
         body {
            padding: 15px;
            margin: 20px auto;
         }
      }
   </style>

</head>

<body>
   <h1>Week 2: Catherine D’Ignazio: 'Data is never a raw, truthful input – and it is never neutral'</h1>
   <p>This interview was very topical for me personally.</p>
   <p>Regularly in my day job, I work with data.</p>
   <p>An everyday responsibility of mine is to adjust processes, analyze, and present this data to stakeholders in such
      a way that it is consistent with past reporting, truthful, and correct.
   </p>
   <p>However, that is often not a tenable task as I am a relatively new addition to the team, and over the years data
      collection, processing, analysis, and presentation methodologies have changed in fairly drastic ways, resulting
      in highly fuzzy data that paints a splotchy mess of a picture when expanded over time
   </p>
   <p>For example, the way demographic data has, as it should, evolved. 5 years ago the options for people to self
      identify their gender was either "Male", "Female", or "Non-binary". Now, we're tracking 10 different selections,
      in any combination.
   </p>
   <p>In my short experience with it, this medium sized organization collecting data has managed to show me everything
      Catherine warns about in her interview. Of her main points:
   </p>
   <h3>Data is never a raw, truthful input:</h3>
   <p>Often I reach out to members of the community asking for updates on how they are doing. While this is a
      "mandatory" data request, often times we don't receive everyone's response for a variety of reasons; they are
      embrassed by their answers, they simply miss it, or they are too busy to take the time to reply.
   </p>
   <h3>Data is not objective or universal:</h3>
   <p>As mentioned above, the way we collect demographic data has changed over the years. This is a good thing, but it
      makes it very difficult to compare data from 5 years ago to data from today. Alongside that, the way data is
      presented changes greatly dependent on the receiving audience. Percentage changes here, word choice of
      explanations, and more. These things change the meaning of data subjectively.
   </p>
   <h3>Data can be used be used for good, but it can also be used for bad:</h3>
   <p>While not happening in my organization, a fear I have for changes organizations as a whole make for tracking
      demographic data that is higher definition for 'othered' peoples is only being tracked for things such as
      marketing. To make the organization 'look good'.
   </p>
   <h3>Data feminism's practices</h3>
   <p>What Catherine brings up, and what data feminism underscores, is that we need to be more thoughtful about our
      approaches to data. This involves critically evaluating who is represented, who is doing the representing, and
      for what purpose.
   </p>
   <p>It's a much-needed perspective shift, one that puts an emphasis on justice and equity while acknowledging the
      emotional and human costs of data. So whether it's updating demographic options or being transparent about how
      and why data is collected, a feminist approach provides a holistic framework for ethical decision-making in data
      science.
   </p>
   <h1>Week 3: The Interplay of Crowdsourcing and Collective Digital Artistry</h1>
   <p>"When Pixels Collide" resonated deeply with my professional experiences.</p>
   <p>Just as I grapple with data's fluidity in my job, this article showcased the fluid nature of collective digital
      creation.</p>
   <p>My role often involves presenting data consistently and truthfully, a task made challenging by evolving
      methodologies. Similarly, Reddit's "Place" experiment, as depicted in the article, was a dynamic canvas, shaped
      and reshaped by countless contributors.</p>
   <p>For instance, the way crowdsourcing has evolved mirrors how demographic data has expanded in granularity over
      time. What was once a simple task now requires nuanced understanding and interpretation.</p>
   <p>Reflecting on Jeff Howe's "The Rise of Crowdsourcing", I see parallels:</p>
   <h3>Crowdsourcing as a Democratic Tool:</h3>
   <p>Just as I've seen data collection methodologies change, crowdsourcing has evolved from a simple concept to a
      complex tool for collaboration and creation.</p>
   <h3>Challenges in Quality and Consistency:</h3>
   <p>Similar to my struggles with presenting consistent data over time, crowdsourced projects like "Place" grapple with
      maintaining quality amidst diverse contributions.</p>
   <h3>The Potential for Misuse:</h3>
   <p>While I worry about data being used superficially to enhance an organization's image, crowdsourcing too can be
      misused, as seen with destructive groups like "The Void".</p>
   <h3>A Call for Ethical Practices:</h3>
   <p>Both the articles emphasize the need for ethical practices. Whether it's ensuring data represents everyone fairly
      or ensuring crowdsourced projects are inclusive and respectful, there's a shared call for responsibility and care.
   </p>
   <p>Ultimately, these readings underscore the importance of approaching digital tools and methodologies with a
      critical and ethical lens, always considering the broader implications of our actions.</p>
   <h1>Week 4: Intertwining Art, Technology, and Humanity</h1>
   <p>Lauren McCarthy's exhibition, 'Someone', in downtown Manhattan, challenges the boundaries between technology,
      personal space, and human interaction, which reminds me of an obscure series of videos from the early 2000s;
      'Surveillance Camera Man'
   </p>
   <p>Both McCarthy and Surveillance Camera Man's work invite a reflection on the absolute <i>everywhereness</i> of
      surveillance and its
      intrusion into personal spaces, albeit in different mediums and methods.</p>
   <p>'Someone' nudges the audience to reflect on technology's role in our personal spaces, while Surveillance Camera
      Man's raw footage serves as a candid reminder of our often overlooked reality of constant surveillance.</p>
   <p>Both projects spark conversations about what privacy means today, and how comfortable we are with being watched.
   </p>
   <h1>Week 5: Digital Emotions Explored</h1>
   <p>"We Feel Fine" highlights the intriguing world of online emotions.</p>
   <p>I've been regularly talking about my day-job, as it's heavily steeped in data work. This project was very
      interesting to me because Kamvar and Harris dive into feelings, capturing a global mood.</p>
   <p>The idea of quantifying "moods" over time and displaying this data is very interesting, considering the complexity
      of emotions and the vast differences in cultural expression across the globe.</p>
   <p>While my work often focuses on more tangible metrics, "We Feel Fine" is just about the exact opposite, focusing on
      the more intangible and nuanced
      spectrum of human sentiment.</p>
   <p>What I found most fun about this reading was thinking about how much further it could go, globalizing the
      experience through translation of different countries' postings and overlaying outputs and news headlines on a
      globe.</p>
   <h1>Week 6: Spotting Hidden Bias in AI</h1>
   <p>This was my presentation week, and it was very interesting (And unfortunately hyper relevant to today's world)</p>
   <p>The article "These Women Tried to Warn Us About AI" by Lorena O'Neill in Rolling Stone talks about how some women,
      like Timnit Gebru, warned us early on about biases in AI.</p>
   <p>O'Neill's story and Gebru's experience show bigger worries about the sneaky biases in AI, which reflect society's
      own biases that unknowingly get added and continue to exponentially ramp during the tech's creation.</p>
   <p>Gebru's unexpected firing from Google, due to her efforts to reveal and lessen AI's biases, highlights the need to
      deal with the dangers of unfettered capitalistic motivations for hyper growth at all costs</p>
   <p>The article ends with a thought-provoking question, wondering about what could have happened if these early
      warnings were listened to. It encourages readers to recognize the quiet but serious effects of biases in AI,
      urging a joint effort to fix and learn from past mistakes.</p>
   <h1>Week 7: Navigating the Data Identity Landscape</h1>
   <p>"The Misleading Name, Metaphor Defiance, and Awesome Potential of 'Personal Data'" deep dives into the relational
      facet of
      data.</p>
   <p>Lately though, I've been researching projects about data consent and ownership. One that has stood out is Solid,
      by Sir Tim
      Berners-Lee. While Sheldrake leans towards a communal asset perspective, Solid emphasizes individual ownership and
      control, focusing on taking the role of data management away from companies.</p>
   <p>This contrast offers a fresh view on data management and privacy, exploring the balance between communal benefit
      and individual rights, a core discussion in the evolving data landscape. Though the two have diverging approaches,
      they both contribute to a deeper discussion of data's
      societal impact and the multifaceted considerations it entails.</p>
</body>

</html>